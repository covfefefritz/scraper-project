
###### 2024-04-06
Installed Scrapy and created a new Scrapy project

Defined the data structure in items.py for scraped articles.
Built a spider to crawl and extract data from financial news articles.
Wrote selectors to target specific elements on web pages for scraping.

This required some debugging and troubleshooting:
    HTML structure differ between sites, some make it more difficult than others to create the Scrapy selectors 

Generalize Spider:
    Implemented logic to distinguish between different news sites in the spider's parse method.
    Parsing functions tailored for specific news sites (parse_zerohedge, parse_cnbc, etc.).


###### 2024-04-07
Data Collection and Scraping:
  - Implemented dynamic naming for output files based on the crawl date to organize collected data over time.

Data Cleaning
  - Developed a `cleaner.py` script to preprocess the scraped data using the pandas library
    - Included functionality to remove HTML tags and entities, normalize text, and handle missing values.
  - Modified the script to accept input filenames as command-line arguments, improving automation and flexibility.

Sentiment Analysis:
  - Integrated NLTK's SentimentIntensityAnalyzer into the cleaning pipeline to assess sentiment of the collected articles.
  - Laid the groundwork for analysis of the tokenized articles

Jupyter Notebook
  - Created for initial data analysis before starting to work with visualization tools like Dash

  ##### 2024-04-14
  Removed jupyter notebook 
    I decided to use streamlit for visualization, i can run the page locally in my browser to fill the same need jupyter notebook had, but I can also deploy the app to showcase it if I wish to do so. 

    Streamlit implementation of app.py, this should probably be broken into different .py scripts as it grows. After I got it working I refactored the code into smaller functions
    I created another script that pushes the generated file to a shared google spreadsheet using the google API, this script is not in the directory for privacy reasons, perhaps I'll add it later 
    and only exclude the credentials file. 

    The crawler is now fully automated with the use of cron, it scrapes, clean, analyze, concatenate, push to gsheets, and app.py reads from the shared google sheet. 

###### 2024-04-15
  Added two more graphs, sentiment and occurence of words over time! 

  Next task: increase the scraper range

  